{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics\n",
    "import sklearn.preprocessing\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data\n",
    "Split into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = sklearn.datasets.load_digits()\n",
    "test_size = 797/1797\n",
    "shuffle = False\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(digits.data, digits.target, test_size=test_size, shuffle=shuffle)\n",
    "\n",
    "assert X_train.shape == (1000, 64), X_train.shape\n",
    "assert Y_test.shape == (797, ), X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labelencoder = sklearn.preprocessing.OneHotEncoder()\n",
    "Y_train_labeled = labelencoder.fit_transform(Y_train.reshape(-1,1)).todense()\n",
    "Y_test_labeled = labelencoder.fit_transform(Y_test.reshape(-1,1)).todense()\n",
    "\n",
    "assert Y_train_labeled.shape == (1000, 10), Y_train_labeled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build PLSR models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAABlCAYAAABtEgn4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\naozBLUmNMbglqTEGtyQ15n87OMjoGCbYvAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7a3e7b9240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[0.70012547, 0.70012547, 0.70012547, 0.70012547, 0.70012547,\n",
       "        0.70012547, 0.70012547, 0.70012547, 0.70012547, 0.70012547,\n",
       "        0.70012547, 0.70012547, 0.70012547, 0.70012547, 0.70012547,\n",
       "        0.70012547, 0.70012547, 0.70012547, 0.70012547, 0.70012547,\n",
       "        0.70012547, 0.70012547, 0.70012547, 0.70012547, 0.70012547,\n",
       "        0.70012547, 0.70012547, 0.70012547, 0.70012547, 0.70012547,\n",
       "        0.70012547, 0.70012547, 0.70012547, 0.70012547, 0.70012547,\n",
       "        0.70012547, 0.70012547, 0.70012547, 0.70012547, 0.70012547,\n",
       "        0.70012547, 0.70012547, 0.70012547, 0.70012547, 0.70012547,\n",
       "        0.70012547, 0.70012547, 0.70012547, 0.70012547, 0.70012547],\n",
       "       [0.76537014, 0.76537014, 0.76537014, 0.76537014, 0.76537014,\n",
       "        0.76537014, 0.76537014, 0.76537014, 0.76537014, 0.76537014,\n",
       "        0.76537014, 0.76537014, 0.76537014, 0.76537014, 0.76537014,\n",
       "        0.76537014, 0.76537014, 0.76537014, 0.76537014, 0.76537014,\n",
       "        0.76537014, 0.76537014, 0.76537014, 0.76537014, 0.76537014,\n",
       "        0.76537014, 0.76537014, 0.76537014, 0.76537014, 0.76537014,\n",
       "        0.76537014, 0.76537014, 0.76537014, 0.76537014, 0.76537014,\n",
       "        0.76537014, 0.76537014, 0.76537014, 0.76537014, 0.76537014,\n",
       "        0.76537014, 0.76537014, 0.76537014, 0.76537014, 0.76537014,\n",
       "        0.76537014, 0.76537014, 0.76537014, 0.76537014, 0.76537014],\n",
       "       [0.78419072, 0.78419072, 0.78419072, 0.78419072, 0.78419072,\n",
       "        0.78419072, 0.78419072, 0.78419072, 0.78419072, 0.78419072,\n",
       "        0.78419072, 0.78419072, 0.78419072, 0.78419072, 0.78419072,\n",
       "        0.78419072, 0.78419072, 0.78419072, 0.78419072, 0.78419072,\n",
       "        0.78419072, 0.78419072, 0.78419072, 0.78419072, 0.78419072,\n",
       "        0.78419072, 0.78419072, 0.78419072, 0.78419072, 0.78419072,\n",
       "        0.78419072, 0.78419072, 0.78419072, 0.78419072, 0.78419072,\n",
       "        0.78419072, 0.78419072, 0.78419072, 0.78419072, 0.78419072,\n",
       "        0.78419072, 0.78419072, 0.78419072, 0.78419072, 0.78419072,\n",
       "        0.78419072, 0.78419072, 0.78419072, 0.78419072, 0.78419072],\n",
       "       [0.86072773, 0.86072773, 0.86072773, 0.86072773, 0.86072773,\n",
       "        0.86072773, 0.86072773, 0.86072773, 0.86072773, 0.86072773,\n",
       "        0.86072773, 0.86072773, 0.86072773, 0.86072773, 0.86072773,\n",
       "        0.86072773, 0.86072773, 0.86072773, 0.86072773, 0.86072773,\n",
       "        0.86072773, 0.86072773, 0.86072773, 0.86072773, 0.86072773,\n",
       "        0.86072773, 0.86072773, 0.86072773, 0.86072773, 0.86072773,\n",
       "        0.86072773, 0.86072773, 0.86072773, 0.86072773, 0.86072773,\n",
       "        0.86072773, 0.86072773, 0.86072773, 0.86072773, 0.86072773,\n",
       "        0.86072773, 0.86072773, 0.86072773, 0.86072773, 0.86072773,\n",
       "        0.86072773, 0.86072773, 0.86072773, 0.86072773, 0.86072773],\n",
       "       [0.85069009, 0.85069009, 0.85069009, 0.85069009, 0.85069009,\n",
       "        0.85069009, 0.85069009, 0.85069009, 0.85069009, 0.85069009,\n",
       "        0.85069009, 0.85069009, 0.85069009, 0.85069009, 0.85069009,\n",
       "        0.85069009, 0.85069009, 0.85069009, 0.85069009, 0.85069009,\n",
       "        0.85069009, 0.85069009, 0.85069009, 0.85069009, 0.85069009,\n",
       "        0.85069009, 0.85069009, 0.85069009, 0.85069009, 0.85069009,\n",
       "        0.85069009, 0.85069009, 0.85069009, 0.85069009, 0.85069009,\n",
       "        0.85069009, 0.85069009, 0.85069009, 0.85069009, 0.85069009,\n",
       "        0.85069009, 0.85069009, 0.85069009, 0.85069009, 0.85069009,\n",
       "        0.85069009, 0.85069009, 0.85069009, 0.85069009, 0.85069009],\n",
       "       [0.87076537, 0.87076537, 0.87076537, 0.87076537, 0.87076537,\n",
       "        0.87076537, 0.87076537, 0.87076537, 0.87076537, 0.87076537,\n",
       "        0.87076537, 0.87076537, 0.87076537, 0.87076537, 0.87076537,\n",
       "        0.87076537, 0.87076537, 0.87076537, 0.87076537, 0.87076537,\n",
       "        0.87076537, 0.87076537, 0.87076537, 0.87076537, 0.87076537,\n",
       "        0.87076537, 0.87076537, 0.87076537, 0.87076537, 0.87076537,\n",
       "        0.87076537, 0.87076537, 0.87076537, 0.87076537, 0.87076537,\n",
       "        0.87076537, 0.87076537, 0.87076537, 0.87076537, 0.87076537,\n",
       "        0.87076537, 0.87076537, 0.87076537, 0.87076537, 0.87076537,\n",
       "        0.87076537, 0.87076537, 0.87076537, 0.87076537, 0.87076537],\n",
       "       [0.86700125, 0.86700125, 0.86700125, 0.86700125, 0.86700125,\n",
       "        0.86700125, 0.86700125, 0.86700125, 0.86700125, 0.86700125,\n",
       "        0.86700125, 0.86700125, 0.86700125, 0.86700125, 0.86700125,\n",
       "        0.86700125, 0.86700125, 0.86700125, 0.86700125, 0.86700125,\n",
       "        0.86700125, 0.86700125, 0.86700125, 0.86700125, 0.86700125,\n",
       "        0.86700125, 0.86700125, 0.86700125, 0.86700125, 0.86700125,\n",
       "        0.86700125, 0.86700125, 0.86700125, 0.86700125, 0.86700125,\n",
       "        0.86700125, 0.86700125, 0.86700125, 0.86700125, 0.86700125,\n",
       "        0.86700125, 0.86700125, 0.86700125, 0.86700125, 0.86700125,\n",
       "        0.86700125, 0.86700125, 0.86700125, 0.86700125, 0.86700125],\n",
       "       [0.8782936 , 0.8782936 , 0.8782936 , 0.8782936 , 0.8782936 ,\n",
       "        0.8782936 , 0.8782936 , 0.8782936 , 0.8782936 , 0.8782936 ,\n",
       "        0.8782936 , 0.8782936 , 0.8782936 , 0.8782936 , 0.8782936 ,\n",
       "        0.8782936 , 0.8782936 , 0.8782936 , 0.8782936 , 0.8782936 ,\n",
       "        0.8782936 , 0.8782936 , 0.8782936 , 0.8782936 , 0.8782936 ,\n",
       "        0.8782936 , 0.8782936 , 0.8782936 , 0.8782936 , 0.8782936 ,\n",
       "        0.8782936 , 0.8782936 , 0.8782936 , 0.8782936 , 0.8782936 ,\n",
       "        0.8782936 , 0.8782936 , 0.8782936 , 0.8782936 , 0.8782936 ,\n",
       "        0.8782936 , 0.8782936 , 0.8782936 , 0.8782936 , 0.8782936 ,\n",
       "        0.8782936 , 0.8782936 , 0.8782936 , 0.8782936 , 0.8782936 ],\n",
       "       [0.88205772, 0.88205772, 0.88205772, 0.88205772, 0.88205772,\n",
       "        0.88205772, 0.88205772, 0.88205772, 0.88205772, 0.88205772,\n",
       "        0.88205772, 0.88205772, 0.88205772, 0.88205772, 0.88205772,\n",
       "        0.88205772, 0.88205772, 0.88205772, 0.88205772, 0.88205772,\n",
       "        0.88205772, 0.88205772, 0.88205772, 0.88205772, 0.88205772,\n",
       "        0.88205772, 0.88205772, 0.88205772, 0.88205772, 0.88205772,\n",
       "        0.88205772, 0.88205772, 0.88205772, 0.88205772, 0.88205772,\n",
       "        0.88205772, 0.88205772, 0.88205772, 0.88205772, 0.88205772,\n",
       "        0.88205772, 0.88205772, 0.88205772, 0.88205772, 0.88205772,\n",
       "        0.88205772, 0.88205772, 0.88205772, 0.88205772, 0.88205772],\n",
       "       [0.89084065, 0.89084065, 0.89084065, 0.89084065, 0.89084065,\n",
       "        0.89084065, 0.89084065, 0.89084065, 0.89084065, 0.89084065,\n",
       "        0.89084065, 0.89084065, 0.89084065, 0.89084065, 0.89084065,\n",
       "        0.89084065, 0.89084065, 0.89084065, 0.89084065, 0.89084065,\n",
       "        0.89084065, 0.89084065, 0.89084065, 0.89084065, 0.89084065,\n",
       "        0.89084065, 0.89084065, 0.89084065, 0.89084065, 0.89084065,\n",
       "        0.89084065, 0.89084065, 0.89084065, 0.89084065, 0.89084065,\n",
       "        0.89084065, 0.89084065, 0.89084065, 0.89084065, 0.89084065,\n",
       "        0.89084065, 0.89084065, 0.89084065, 0.89084065, 0.89084065,\n",
       "        0.89084065, 0.89084065, 0.89084065, 0.89084065, 0.89084065]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_components = 50\n",
    "def build_plsr(X, Y):\n",
    "    assert X.shape[0] == Y.shape[0]\n",
    "    m = PLSRegression(n_components=train_components)\n",
    "    m.fit(X, Y)\n",
    "    return m\n",
    "\n",
    "models = [ build_plsr(X_train[0:rows_used,:], Y_train_labeled[0:rows_used]) for rows_used in range(100, 1100, 100) ]\n",
    "assert len(models) == 10\n",
    "\n",
    "results = numpy.ndarray(shape=(len(models), train_components))\n",
    "for x_idx, predict_components in enumerate(range(1, train_components+1)):\n",
    "    for y_idx, model in enumerate(models):\n",
    "        Y_pred = model.predict(X_test).argmax(axis=1)\n",
    "        #print(Y_test.shape, Y_test[0:5])\n",
    "        #print(Y_pred.shape, Y_pred[0:5])\n",
    "\n",
    "        # classification accuracy\n",
    "        a = sklearn.metrics.accuracy_score(Y_test, Y_pred)\n",
    "        # L00_iris_pcr_plsr_ALT2.py \n",
    "        results[y_idx, x_idx] = a\n",
    "\n",
    "plt.imshow(results, cmap='hot', interpolation='nearest')\n",
    "plt.show()\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST data in scikit-learn consists of 1797 objects (instances). Split the data into a training set (first 1000 rows) and test set (last 797 rows). The test set will serve as “unseen” data for which we will predict the classes. Don’t worry about the balance of classes in the training set; their proportions stay very similar with increasing number of rows.\n",
    "\n",
    "\n",
    "Compute 10 PLSR models, each with 50 components.\n",
    "        Use the first 100 rows of the training set to compute the first PLSR model.\n",
    "        Use the first 200 rows of the training set to compute the second PLSR model.\n",
    "        Use the first 300 rows of the training set to compute the third PLSR model.\n",
    "        etc\n",
    "        \n",
    "For each of the 10 models, predict the classes of the unseen digits using 1 component, 2 components, 3 components, etc. and compute the classification accuracy (which should be value between 0 and 1, or between 0 and 100 if computed as a percentage) from the predictions. Store the results in a (10 x 50) numpy array or pandas data frame.\n",
    "\n",
    "\n",
    "Plot a heatmap of the results using Python plotting packages matplotlib (Lenker til en ekstern side.)Lenker til en ekstern side. or seaborn (Lenker til en ekstern side.)Lenker til en ekstern side.. See below what it should look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "a = np.random.random((16, 16))\n",
    "plt.imshow(a, cmap='hot', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Does the number of training objects matter for prediction performance?\n",
    "* Do the number of components used in the prediction matter for prediction performance?\n",
    "* Which combination of the number of training objects and number of components used for prediction would you use?\n",
    "* Do you think the accuracy achieved with PLSR may be further improved using other classification methods?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
